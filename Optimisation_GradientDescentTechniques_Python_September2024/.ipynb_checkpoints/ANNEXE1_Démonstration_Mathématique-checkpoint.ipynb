{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ffd809-2c4b-433e-b627-070ce3bec18d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **Convexité et Lissité de la cross-entropy binaire**\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391aaa0-3c91-4ed3-9f75-aa94854c6dea",
   "metadata": {},
   "source": [
    "S'inspirant de la démonstration faite en classe pour l'erreur quadratique moyenne, j'essaie ici de faire la démonstration mathématique de la convexité et de la lissité de la fonction cross-entropy binaire qui a été utilisée dans ce travail comme fonction de perte pour régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bac6db-a52b-46f7-aabb-06c80c88c128",
   "metadata": {},
   "source": [
    "# 1. Convexité de la cross-entropy binaire\n",
    "\n",
    "## Formulation mathématique la cross-entropy binaire\n",
    "\n",
    "La fonction de coût \"cross-entropy binaire\" est donnée par :\n",
    "\n",
    "$$L(\\theta) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $y_i \\in \\{0,1\\}$ est la vraie étiquette,\n",
    "- $\\hat{y}_i = \\sigma(z_i)$ est la prédiction du modèle, où $z_i = \\theta^T x_i$, et $\\sigma(z) = \\frac{1}{1+e^{-z}}$ est la fonction sigmoïde,\n",
    "- $\\theta$ représente les paramètres du modèle.\n",
    "\n",
    "## Démonstration de la convexité\n",
    "\n",
    "Une fonction $f$ est convexe si et seulement si sa matrice hessienne (la matrice des dérivées secondes) est semi-définie positive, c'est-à-dire que pour tout vecteur $v$, $v^T H v \\geq 0$, où $H$ est la matrice hessienne de $f$.\n",
    "\n",
    "Pour montrer que la cross-entropy binaire est convexe, nous allons d'abord calculer son gradient puis sa hessienne.\n",
    "\n",
    "### Gradient de la cross-entropy binaire\n",
    "\n",
    "Le gradient de la fonction de coût de la cross-entropy binaire par rapport aux paramètres $\\theta$ est donné par :\n",
    "\n",
    "$$\\frac{\\partial L(\\theta)}{\\partial \\theta} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)x_i$$\n",
    "\n",
    "où :\n",
    "\n",
    "$\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1+e^{-z_i}}$, avec $z_i = \\theta^T x_i$.\n",
    "\n",
    "### Hessienne de la cross-entropy\n",
    "\n",
    "La Hessienne est la dérivée seconde de la fonction de coût par rapport à $\\theta$. Pour obtenir la Hessienne, nous devons différencier le gradient par rapport à $\\theta$.\n",
    "\n",
    "Le gradient est de la forme $(\\hat{y}_i - y_i)x_i$, où $\\hat{y}_i = \\sigma(z_i)$. En différentiant à nouveau, on obtient :\n",
    "\n",
    "$$H(\\theta) = \\frac{\\partial^2 L(\\theta)}{\\partial \\theta^2} = \\frac{1}{N} \\sum_{i=1}^N \\hat{y}_i(1-\\hat{y}_i)x_i x_i^T$$\n",
    "\n",
    "Cette expression représente la Hessienne de la cross-entropy.\n",
    "\n",
    "### Semi-définie positivité de la Hessienne\n",
    "\n",
    "Pour que la fonction soit convexe, il faut que la Hessienne soit semi-définie positive, c'est-à-dire que pour tout vecteur $v$, on ait $v^T H(\\theta)v \\geq 0$.\n",
    "\n",
    "Or, pour tout vecteur $v \\in \\mathbb{R}^d$, on a :\n",
    "\n",
    "$v^T (x_i x_i^T) v = (v^T x_i)(x_i^T v) = (v^T x_i)^2 \\geq 0$\n",
    "\n",
    "Donc, La forme de la Hessienne $H(\\theta) = \\sum_i \\hat{y}_i(1-\\hat{y}_i)x_i x_i^T$ est une somme pondérée de matrices semi-définies positives ($x_i x_i^T$) avec des poids $\\hat{y}_i(1-\\hat{y}_i)$, qui sont toujours non-négatifs car $\\hat{y}_i \\in (0,1)$.\n",
    "\n",
    "Par conséquent, la Hessienne est semi-définie positive, et donc la fonction de coût de **la cross-entropy binaire est convexe**.\n",
    "\n",
    "# 2. Lissité de la cross-entropy binaire\n",
    "\n",
    "## Définition de la lissité\n",
    "\n",
    "Une fonction est dite lisse si son gradient est Lipschitz-continu, c'est-à-dire qu'il existe une constante $L > 0$ telle que pour tout $\\theta_1$ et $\\theta_2$ :\n",
    "\n",
    "$$\\|\\nabla L(\\theta_1) - \\nabla L(\\theta_2)\\| \\leq L \\|\\theta_1 - \\theta_2\\|$$\n",
    "\n",
    "Nous allons démontrer que la fonction de coût de la cross-entropy binaire satisfait cette propriété.\n",
    "\n",
    "## Lissité via la Hessienne\n",
    "\n",
    "La constante de Lipschitz $L$ est liée à la norme spectrale maximale de la Hessienne, c'est-à-dire la valeur propre maximale de $H(\\theta)$.\n",
    "\n",
    "La Hessienne de la cross-entropy binaire est donnée par :\n",
    "\n",
    "$$H(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\hat{y}_i(1-\\hat{y}_i)x_i x_i^T$$\n",
    "\n",
    "Comme $\\hat{y}_i(1-\\hat{y}_i) \\leq \\frac{1}{4}$ pour tout $i$ (car la dérivée de la fonction sigmoïde est maximisée à 0.25), on peut majorer la norme spectrale de la Hessienne par :\n",
    "\n",
    "$$\\|H(\\theta)\\| \\leq \\frac{1}{4N} \\sum_{i=1}^N \\|x_i x_i^T\\| = \\frac{1}{4N} \\sum_{i=1}^N \\|x_i\\|^2$$\n",
    "\n",
    "Ainsi, la constante de Lipschitz $L$ est bornée par :\n",
    "\n",
    "$$L \\leq \\frac{1}{4N} \\sum_{i=1}^N \\|x_i\\|^2$$\n",
    "\n",
    "Cela signifie que la fonction de coût de la cross-entropy binaire est Lipschitz continue avec une constante $L$, **et donc elle est lisse**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
